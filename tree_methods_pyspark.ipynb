{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificando o conservante que leva alguns lotes da ração de uma empresa estragarem  \n",
    "\n",
    "## Descrição do problema \n",
    "\n",
    "Alguns lotes das rações para cachorro de uma empresa têm estragado precocemente e aparentemente um dos conservantes usados pode ser a causa. Na composição da ração são utilizados 4 conservantes, chamados de A, B, C, e D, e as quantidades desses podem variar nos diferentes lotes.  \n",
    "\n",
    "A base de dados da empresa contém informações sobre as quantidades dos conservantes utilizados em cada lote e indica se o lote estragou ou não. \n",
    "\n",
    "O objetivo dessa análise é utilizar Machine Learning para prever os lotes que irão estragar e pelo resultado da modelagem identificar o conservante que está relacionado como o apodrecimento dos lotes.\n",
    "\n",
    "O algorítmo Random Forest foi utilizado na classificação e o principal componente na determinação do apodrecimento da ração foi o conservante C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos dados e pré-analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('tree_dog').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# carrega os dados\n",
    "data = spark.read.csv('dog_food.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "|summary|                 A|                 B|                 C|                 D|            Spoiled|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "|  count|               490|               490|               490|               490|                490|\n",
      "|   mean|  5.53469387755102| 5.504081632653061| 9.126530612244897| 5.579591836734694| 0.2857142857142857|\n",
      "| stddev|2.9515204234399057|2.8537966089662063|2.0555451971054275|2.8548369309982857|0.45221563164613465|\n",
      "|    min|                 1|                 1|               5.0|                 1|                0.0|\n",
      "|    max|                10|                10|              14.0|                10|                1.0|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrupa as variáveis para utilizá-las na classificação\n",
    "assembler = VectorAssembler(inputCols=['A', 'B', 'C', 'D'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleciona a coluna com as variáveis preditivas ('features') e a coluna com a variável resposta ('Spoiled')\n",
    "final_data = output.select('Spoiled', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|Spoiled|           features|\n",
      "+-------+-------------------+\n",
      "|    1.0| [4.0,2.0,12.0,3.0]|\n",
      "|    1.0| [5.0,6.0,12.0,7.0]|\n",
      "|    1.0| [6.0,2.0,13.0,6.0]|\n",
      "|    1.0| [4.0,2.0,12.0,1.0]|\n",
      "|    1.0| [4.0,2.0,12.0,3.0]|\n",
      "|    1.0|[10.0,3.0,13.0,9.0]|\n",
      "|    1.0| [8.0,5.0,14.0,5.0]|\n",
      "|    1.0| [5.0,8.0,12.0,8.0]|\n",
      "|    1.0| [6.0,5.0,12.0,9.0]|\n",
      "|    1.0| [3.0,3.0,12.0,1.0]|\n",
      "|    1.0| [9.0,8.0,11.0,3.0]|\n",
      "|    1.0|[1.0,10.0,12.0,3.0]|\n",
      "|    1.0|[1.0,5.0,13.0,10.0]|\n",
      "|    1.0|[2.0,10.0,12.0,6.0]|\n",
      "|    1.0|[1.0,10.0,11.0,4.0]|\n",
      "|    1.0| [5.0,3.0,12.0,2.0]|\n",
      "|    1.0| [4.0,9.0,11.0,8.0]|\n",
      "|    1.0| [5.0,1.0,11.0,1.0]|\n",
      "|    1.0|[4.0,9.0,12.0,10.0]|\n",
      "|    1.0| [5.0,8.0,10.0,9.0]|\n",
      "+-------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo de classificação Random Forest\n",
    "rf_dog = RandomForestClassifier(labelCol='Spoiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide os dados entre treino e teste\n",
    "train_data, test_data = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajusta o modelo nos dados de treino\n",
    "rf_fit = rf_dog.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predição nos dados de teste\n",
    "pred_dog = rf_fit.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+----------+\n",
      "|Spoiled|          features|       rawPrediction|         probability|prediction|\n",
      "+-------+------------------+--------------------+--------------------+----------+\n",
      "|    0.0| [1.0,3.0,9.0,8.0]|[19.8231916982397...|[0.99115958491198...|       0.0|\n",
      "|    0.0|[1.0,5.0,8.0,10.0]|[18.9795244910941...|[0.94897622455470...|       0.0|\n",
      "|    0.0| [1.0,6.0,8.0,3.0]|[19.9795244910941...|[0.99897622455470...|       0.0|\n",
      "|    0.0| [1.0,7.0,8.0,2.0]|[19.9795244910941...|[0.99897622455470...|       0.0|\n",
      "|    0.0|[1.0,8.0,7.0,10.0]|[19.8947368421052...|[0.99473684210526...|       0.0|\n",
      "|    0.0| [1.0,8.0,8.0,7.0]|[19.7231368736783...|[0.98615684368391...|       0.0|\n",
      "|    0.0| [1.0,8.0,8.0,8.0]|[19.7231368736783...|[0.98615684368391...|       0.0|\n",
      "|    0.0| [1.0,9.0,9.0,7.0]|[19.7231368736783...|[0.98615684368391...|       0.0|\n",
      "|    0.0| [2.0,1.0,8.0,9.0]|[19.3947916666666...|[0.96973958333333...|       0.0|\n",
      "|    0.0| [2.0,1.0,8.0,9.0]|[19.3947916666666...|[0.96973958333333...|       0.0|\n",
      "|    0.0|[2.0,1.0,10.0,7.0]|[17.1884521389856...|[0.85942260694928...|       0.0|\n",
      "|    0.0| [2.0,2.0,8.0,1.0]|[19.9947916666666...|[0.99973958333333...|       0.0|\n",
      "|    0.0| [2.0,2.0,9.0,8.0]|[19.8231916982397...|[0.99115958491198...|       0.0|\n",
      "|    0.0| [2.0,3.0,6.0,9.0]|[19.8394518608414...|[0.99197259304207...|       0.0|\n",
      "|    0.0| [2.0,3.0,9.0,3.0]|[19.9947916666666...|[0.99973958333333...|       0.0|\n",
      "|    0.0| [2.0,5.0,8.0,3.0]|[19.9795244910941...|[0.99897622455470...|       0.0|\n",
      "|    0.0| [2.0,5.0,9.0,5.0]|[19.9632643284925...|[0.99816321642462...|       0.0|\n",
      "|    0.0| [2.0,6.0,6.0,9.0]|[19.8241846852689...|[0.99120923426344...|       0.0|\n",
      "|    0.0| [2.0,6.0,8.0,8.0]|[19.8079245226672...|[0.99039622613336...|       0.0|\n",
      "|    0.0| [2.0,8.0,6.0,9.0]|[19.7393970362800...|[0.98696985181400...|       0.0|\n",
      "+-------+------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_dog.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator(labelCol='Spoiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9798934108527131"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computa a área sob a curva ROC\n",
    "AUC = my_eval.evaluate(pred_dog)\n",
    "AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinação da variável de maior importância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(4, {0: 0.0224, 1: 0.02, 2: 0.9325, 3: 0.0252})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computa as importâncias relativas entre as variáveis\n",
    "rf_fit.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O composto responsável por estragar a comida é portanto o composto C. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
